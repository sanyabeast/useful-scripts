# hashnamer.py

Hashnamer - Rename files using their content hash to eliminate naming conflicts and duplicates.

This script recursively traverses a directory and renames files based on their content's cryptographic hash. This is particularly useful for organizing media libraries or ensuring unique filenames when dealing with potentially duplicated files from different sources.

The script offers options to delete duplicate files, filter by file extension, choose the hashing algorithm (MD5, SHA256, or SHA512), track duplicates globally across subdirectories, and control the length of the generated hash name.  **Use with caution as renaming/deleting files is irreversible.**

**Important:** This script modifies your filesystem. It's highly recommended to test it on a small sample directory before running it on larger datasets.

## Features:

*   **Content-Based Renaming:** Files are renamed based on their content hash, ensuring uniqueness even if filenames were identical.
*   **Duplicate Handling:**  Optionally delete or skip duplicate files identified by their hashes.
*   **File Extension Filtering:** Process only specific file types (e.g., images).
*   **Hashing Algorithm Choice:** Supports MD5, SHA256, and SHA512 hashing algorithms.
*   **Global Duplicate Tracking:**  Track duplicates across all subdirectories within the target directory.
*   **Hash Truncation:** Control the length of the generated hash name for brevity.

## Components & Logic:

1.  **`generate_random_string(length)`**: Generates a random alphanumeric string (not used in current script).
2.  **`generate_file_hash_md5(file_path)`, `generate_file_hash_sha256(file_path)`, `generate_file_hash_sha512(file_path)`**: Functions to calculate the MD5, SHA-256, or SHA-512 hash of a file's content.  They read the file in chunks to handle large files efficiently.
3.  **`rename_files(directory, delete_dupes=False, extensions=None, method="md5", global_dupes=False, trim_name=64)`**: The core function that performs the renaming process:
    *   Recursively walks through the specified directory using `os.walk()`. 
    *   Filters files based on provided extensions.
    *   Calculates the hash of each file's content using the selected hashing method.
    *   Checks if a file with the same hash already exists in the `hash_to_file` dictionary:
        *   If it exists and `delete_dupes` is True, the duplicate file is deleted.
        *   If it exists and `delete_dupes` is False, the duplicate file is skipped.
    *   If the hash is unique, a new filename is generated by truncating the hash to the specified length (`trim_name`), keeping the original extension, and renaming the file using `os.rename()`. 
    *   Tracks renamed files, deleted duplicates, skipped duplicates, and skipped files for statistics.
4.  **`main()`**: Parses command-line arguments, validates input (directory existence), prints a summary of the operation parameters, and calls `rename_files()` to start the renaming process.

## Assumptions & Requirements:

*   **Python 3:** The script requires Python 3 to execute. 
*   **Operating System:**  Should work on any OS with Python 3 and standard libraries (Windows, macOS, Linux).
*   **File Permissions:** The script needs read and write permissions for the target directory.
*   **Hashing Algorithm Availability:**  The `hashlib` module is part of the Python standard library, so no external dependencies are required.

## Limitations:

*   **No Error Handling for File Access Issues:** The script doesn't explicitly handle potential errors like file access denied or corrupted files. These could cause unexpected behavior or crashes.
*   **Potential Name Collisions (Extremely Unlikely):** While extremely unlikely, hash collisions are theoretically possible.  This would result in two different files having the same hash and potentially being renamed to the same name.
*   **No Undo Functionality:** The script does not provide a way to undo renaming or deletion operations. **Back up your data before running this script.**

## Usage:

```bash
python hashnamer.py /path/to/directory [delete_dupes] [extensions] [method] [global_dupes] [trim_name]
```

### Arguments:

*   `<directory_path>`: (Required) The path to the directory you want to process.
*   `[delete_dupes]` : (Optional, default: `false`) Set to `true` to delete duplicate files.  If set to `false`, duplicates will be skipped.
*   `[extensions]` : (Optional, default: all files) A comma-separated list of file extensions to process (e.g., `'jpg,png,gif'`).
*   `[method]` : (Optional, default: `md5`) The hashing algorithm to use:  `md5`, `sha256`, or `sha512`.
*   `[global_dupes]` : (Optional, default: `false`) Set to `true` to track duplicates across all subdirectories. If set to `false`, duplicate tracking is limited to each individual directory.
*   `[trim_name]` : (Optional, default: `64`) The length of the hash string to use for the new filenames.

### Examples:

1.  **Rename all files in `/home/user/images` using MD5 hashing:**
    ```bash
    python hashnamer.py /home/user/images
    ```

2.  **Rename only JPG and PNG files in `/media/backup`, delete duplicates, use SHA256 hashing, and trim the hash to 32 characters:**
    ```bash
    python hashnamer.py /media/backup true jpg,png sha256 false 32
    ```

3.  **Rename files in `/documents` using SHA512 hashing, track duplicates globally across all subdirectories:**
    ```bash
    python hashnamer.py /documents global_dupes true sha512
    ```

## Statistics Output:

The script prints a summary of its actions after processing the directory, including:

*   Total files renamed
*   Total files skipped (due to unsupported extensions)
*   Total duplicate files removed
*   Total duplicate files skipped

**Disclaimer:** Use this script at your own risk. Always back up your data before running any script that modifies your filesystem.
