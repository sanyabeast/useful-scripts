# safetensors-to-gguf

This Python script converts a SafeTensors file to the GGUF format.

**Usage:**

```bash
python script.py <input_safetensors_path> <output_gguf_path> [quantization_type]
```

* **`<input_safetensors_path>`:** Path to the input SafeTensors file.
* **`<output_gguf_path>`:** Path to save the output GGUF file.
* **`[quantization_type]`:** Optional quantization type to apply. Supported types are "q4" (4-bit) and "q3" (3-bit). If not specified, no quantization is applied.

**Example:**

```bash
python script.py model.safetensors model.gguf q4
```

This will convert `model.safetensors` to `model.gguf` with 4-bit quantization.

**Explanation:**

The script loads tensors from the SafeTensors file using `safetensors.torch.load_file()`. It then writes a GGUF header followed by metadata, tensor names, shapes, data types, and tensor data to the output file.


**Note:** 

* This script provides a basic example of converting SafeTensors to GGUF. 
* You need to customize the GGUF header and metadata according to your specific requirements.
* The `quantize_tensor()` function is currently a placeholder and needs to be replaced with actual quantization logic for your desired data type.

**Dependencies:**

* `safetensors` (Install using `pip install safetensors`)
